{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7274092-7094-4440-b91c-178196a1348c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  Configuração do Catálogo e Estrutura Inicial\n",
    "\n",
    "> **Nota:**  \n",
    "> Devido à instrução de que **apenas dois notebooks** poderiam ser enviados para esta atividade correspondentes às camadas **Bronze** e **Silver** do modelo de arquitetura **Medalhão**, a etapa de **setup inicial** (criação de catálogo, schemas e volume) foi incluída neste notebook **`01_land_to_bronze`**.  \n",
    ">\n",
    "> Em um ambiente de produção, ou caso houvesse permissão para o envio de um terceiro arquivo, essa etapa ficaria em um notebook separado de configuração (por exemplo, `00_setup_environment.ipynb`), executado uma única vez antes do fluxo de ingestão.  \n",
    ">\n",
    "> No entanto, para garantir a **reprodutibilidade** e o **funcionamento completo** do pipeline apenas com os dois arquivos exigidos, optou-se por incluir aqui os comandos:\n",
    "> - `CREATE CATALOG IF NOT EXISTS medalhao;`\n",
    "> - `USE CATALOG medalhao;`\n",
    "> - `CREATE SCHEMA IF NOT EXISTS bronze;`\n",
    "> - `CREATE SCHEMA IF NOT EXISTS silver;`\n",
    "> - `USE SCHEMA default; CREATE VOLUME IF NOT EXISTS landing;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cdc873e-3481-4a9f-9e89-7264b585c757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751c33f9-6745-4401-af5a-6a2c9190142a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS medalhao;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "933ae808-c341-4dae-a3f8-9da14d36e310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG medalhao;\n",
    "    \n",
    "CREATE SCHEMA IF NOT EXISTS bronze;\n",
    "CREATE SCHEMA IF NOT EXISTS silver;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94700873-1faa-4efb-9a6f-b528e70bf46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE SCHEMA default;\n",
    "CREATE VOLUME IF NOT EXISTS landing;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be73e45e-121c-4eb5-9dab-c1188657326e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CONFIGURAÇÕES INICIAIS E INGESTÃO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beda4d70-e6cb-4f24-b3dc-42f73e346f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Objetivo:**  \n",
    "Esta célula importa as bibliotecas essenciais para a manipulação de dados, conexão com o Spark e integração com APIs externas.  \n",
    "\n",
    "**Descrição dos principais imports:**\n",
    "- `pyspark.sql` → Criação de sessões Spark e manipulação de DataFrames distribuídos.  \n",
    "- `datetime` → Manipulação de datas e formatação de períodos.  \n",
    "- `pyspark.sql.functions` → Funções nativas do Spark, como `current_timestamp`, `col`, `max` e agregações diversas.  \n",
    "- `requests` → Realiza requisições HTTP para a API do Banco Central (cotação do dólar).  \n",
    "- `pandas` → Suporte à conversão de dados entre estruturas locais (Pandas DataFrame) e distribuídas (Spark DataFrame).  \n",
    "- `time` → Utilizado para controlar intervalos e tentativas em requisições com retries.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6633d739-d49e-4900-98a5-4da7856d3169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import current_timestamp, col, max\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "564cecf5-50b6-40bf-a306-a4441f58feae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Objetivo:**  \n",
    "Definir as variáveis básicas da arquitetura Medalhão e apontar o ambiente Spark para o catálogo e schema corretos antes das operações de leitura e escrita.\n",
    "\n",
    "**O que a célula faz:**\n",
    "- `catalog_name`, `bronze_db_name`, `silver_db_name` → definem os nomes das camadas lógicas da arquitetura Medalhão.  \n",
    "- `landing_volume` → representa o volume físico usado para armazenar os arquivos brutos (camada *Landing*).  \n",
    "- `base_path` → constrói dinamicamente o caminho para o volume de origem dentro do Unity Catalog (`/Volumes/medalhao/default/landing`).  \n",
    "- `spark.sql(\"USE CATALOG ...\")` → muda o contexto do Spark para o catálogo principal `medalhao`.  \n",
    "- `spark.sql(\"USE SCHEMA ...\")` → seleciona o schema `bronze`, garantindo que todas as tabelas criadas a partir deste ponto sejam registradas nessa camada.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "772b20e1-82b5-4027-af22-4f231d8f13e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"medalhao\"\n",
    "bronze_db_name = \"bronze\"\n",
    "silver_db_name = \"silver\"\n",
    "\n",
    "landing_volume = \"landing\"\n",
    "\n",
    "base_path = f\"/Volumes/{catalog_name}/default/{landing_volume}\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {bronze_db_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "707889e8-1818-4163-a070-9def1cedcc0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Objetivo:**  \n",
    "Ler os arquivos CSV do volume *Landing*, adicionar a coluna de controle `ingestion_timestamp` e gravar os dados na camada **Bronze** da arquitetura Medalhão, criando as tabelas correspondentes no catálogo `medalhao`.\n",
    "\n",
    "**Descrição detalhada:**\n",
    "- **`file_path`** → monta o caminho completo até o arquivo CSV dentro do volume `/Volumes/medalhao/default/landing`.  \n",
    "- **`full_table_name`** → define o nome completo da tabela no formato `<catálogo>.<schema>.<tabela>`.  \n",
    "- **`spark.read.format(\"csv\")`** → lê o arquivo CSV com cabeçalho e infere automaticamente o schema das colunas.  \n",
    "- **`withColumn(\"ingestion_timestamp\", current_timestamp())`** → adiciona uma coluna com o instante exato de carregamento do dado, útil para auditoria e controle de versões.  \n",
    "- **`df.write.mode(\"overwrite\").saveAsTable(full_table_name)`** → grava os dados como uma **tabela gerenciada** no Unity Catalog, substituindo o conteúdo anterior (modo *overwrite*).  \n",
    "- O `try/except` garante que possíveis erros de leitura ou escrita sejam tratados e exibidos no log.\n",
    "\n",
    "**Observações:**\n",
    "- As tabelas são salvas no formato **Delta** por padrão (Databricks gerencia automaticamente `saveAsTable` como Delta).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1dd00cf-53fb-4865-bd8b-a73b7e510387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_csv(file_name: str, table_name: str):\n",
    "    file_path = f\"{base_path}/{file_name}\"\n",
    "    full_table_name = f\"{catalog_name}.{bronze_db_name}.{table_name}\"\n",
    "    \n",
    "    print(f\"\\nIniciando ingestão de {file_name} → {full_table_name}\")\n",
    "    \n",
    "    try:\n",
    "        df = (\n",
    "            spark.read\n",
    "            .format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .load(file_path)\n",
    "        )\n",
    "        df = df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        \n",
    "        df.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "        \n",
    "        print(f\"Tabela criada: {full_table_name} ({df.count()} registros)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ingerir {file_name}: {e}\")\n",
    "\n",
    "ingest_csv(\"olist_customers_dataset.csv\", \"ft_consumidores\")\n",
    "ingest_csv(\"olist_geolocation_dataset.csv\", \"ft_geolocalizacao\")\n",
    "ingest_csv(\"olist_order_items_dataset.csv\", \"ft_itens_pedidos\")\n",
    "ingest_csv(\"olist_order_payments_dataset.csv\", \"ft_pagamentos_pedidos\")\n",
    "ingest_csv(\"olist_order_reviews_dataset.csv\", \"ft_avaliacoes_pedidos\")\n",
    "ingest_csv(\"olist_orders_dataset.csv\", \"ft_pedidos\")\n",
    "ingest_csv(\"olist_products_dataset.csv\", \"ft_produtos\")\n",
    "ingest_csv(\"olist_sellers_dataset.csv\", \"ft_vendedores\")\n",
    "ingest_csv(\"product_category_name_translation.csv\", \"dm_categoria_produtos_traducao\")\n",
    "\n",
    "print(\"\\nIngestão da camada Bronze concluída\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ed959cf-67c5-48ba-87f0-1a35ae5f6984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Objetivo:**  \n",
    "Permitir uma verificação rápida das tabelas ingeridas na camada Bronze, exibindo uma amostra dos dados carregados a partir dos arquivos CSV.\n",
    "\n",
    "**Descrição da função:**\n",
    "- **`preview_tables()`** → percorre a lista de tabelas definidas em `bronze_tables` e exibe as primeiras linhas de cada uma.  \n",
    "- **Parâmetro `limit_rows`** → define quantas linhas serão exibidas (por padrão, 5).  \n",
    "- **`spark.table(full_table_name)`** → lê cada tabela diretamente do catálogo `medalhao.bronze`.  \n",
    "- **`display(df)`** → exibe os registros no formato visual interativo do Databricks, facilitando a inspeção manual.  \n",
    "- O bloco `try/except` garante que, se alguma tabela não existir ou ocorrer erro de leitura, o processo continue normalmente para as demais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cb01fd-5867-495c-982b-2409cc2a6f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preview_tables(tables: list, limit_rows: int = 5):\n",
    "    \"\"\"\n",
    "    Lê cada tabela da camada Bronze e exibe as primeiras linhas (limit configurável).\n",
    "    \"\"\"\n",
    "    for table in tables:\n",
    "        full_table_name = f\"{catalog_name}.{bronze_db_name}.{table}\"\n",
    "        print(f\"\\n\uD83D\uDCD8 Exibindo primeiras {limit_rows} linhas da tabela: {full_table_name}\")\n",
    "        try:\n",
    "            df = spark.table(full_table_name).limit(limit_rows)\n",
    "            display(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao exibir {full_table_name}: {e}\")\n",
    "\n",
    "bronze_tables = [\n",
    "    \"ft_consumidores\",\n",
    "    \"ft_geolocalizacao\",\n",
    "    \"ft_itens_pedidos\",\n",
    "    \"ft_pagamentos_pedidos\",\n",
    "    \"ft_avaliacoes_pedidos\",\n",
    "    \"ft_pedidos\",\n",
    "    \"ft_produtos\",\n",
    "    \"ft_vendedores\",\n",
    "    \"dm_categoria_produtos_traducao\"\n",
    "]\n",
    "\n",
    "preview_tables(bronze_tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc95f133-27de-40ed-b7b6-8510cf04ede7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Objetivo:**  \n",
    "Preparar as variáveis e formatos de data que serão utilizados na etapa de ingestão da cotação do dólar, obtida via API do Banco Central.\n",
    "\n",
    "**Descrição das variáveis:**\n",
    "- **`table_full_name`** → define o nome completo da tabela que armazenará as cotações, seguindo o padrão do catálogo e schema do projeto (`medalhao.bronze.dm_cotacao_dolar`).  \n",
    "  Essa tabela será responsável por armazenar as informações de cotação diária do dólar em formato Delta.  \n",
    "- **`API_OUTPUT_FORMAT`** → especifica o formato exigido pela API do Banco Central (`%m-%d-%Y`), ou seja, *mês-dia-ano*.  \n",
    "- **`POSSIBLE_INPUT_FORMATS`** → lista de formatos alternativos aceitos como entrada (ex.: `YYYY-MM-DD`, `DD/MM/YYYY`, `MM-DD-YYYY`). Esses formatos serão utilizados para converter automaticamente as datas antes da chamada à API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d793a853-934f-42ca-b1b5-d98290fde972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_full_name = f\"{catalog_name}.{bronze_db_name}.dm_cotacao_dolar\"\n",
    "\n",
    "API_OUTPUT_FORMAT = \"%m-%d-%Y\" \n",
    "POSSIBLE_INPUT_FORMATS = [\"%Y-%m-%d\", \"%d/%m/%Y\", \"%m-%d-%Y\", \"%Y/%m/%d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53698109-9e12-4818-939b-70f97b2a57bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Objetivo:**  \n",
    "Garantir que as datas de início e fim utilizadas na consulta da API do Banco Central (PTAX) estejam no formato correto (`MM-DD-YYYY`), conforme exigido pelo endpoint oficial.\n",
    "\n",
    "**Descrição da função:**\n",
    "- **`normalize_widget_date(date_str)`** → tenta converter o valor de data recebido de um *widget* ou variável manual para o formato exigido pela API.  \n",
    "  - Se a data estiver vazia, lança um `ValueError`.  \n",
    "  - Se o formato não corresponder a nenhum dos esperados em `POSSIBLE_INPUT_FORMATS`, também gera um erro informativo.  \n",
    "  - Em caso de sucesso, retorna a data convertida no formato `MM-DD-YYYY`.\n",
    "\n",
    "**Fluxo de execução da célula:**\n",
    "1. Define a data inicial (`raw_start_date = \"2010-01-01\"`) e a data final como a **data atual do sistema** (`datetime.now()`).\n",
    "2. Exibe no console as datas utilizadas para a extração completa (*modo histórico total*).\n",
    "3. Converte ambas as datas usando a função `normalize_widget_date()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c768ac0e-684e-444b-a071-a1ffcb6d56d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_widget_date(date_str: str) -> str:\n",
    "    \"\"\"Tenta converter a string de data de entrada para o formato da API (MM-DD-YYYY).\"\"\"\n",
    "    if not date_str:\n",
    "        raise ValueError(f\"Erro: O valor do widget está vazio ('{date_str}').\")\n",
    "        \n",
    "    for fmt in POSSIBLE_INPUT_FORMATS:\n",
    "        try:\n",
    "            dt_obj = datetime.strptime(date_str, fmt)\n",
    "            return dt_obj.strftime(API_OUTPUT_FORMAT)\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "    raise ValueError(\n",
    "        f\"Erro: O valor do widget '{date_str}' não corresponde a nenhum dos formatos de data esperados: \"\n",
    "        f\"{', '.join(POSSIBLE_INPUT_FORMATS)}\"\n",
    "    )\n",
    "\n",
    "raw_start_date = \"2010-01-01\" \n",
    "print(f\"Modo Histórico Total: Iniciando extração em: {raw_start_date}\")\n",
    "\n",
    "raw_end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(f\"Data Final: Usando data atual ({raw_end_date})\")\n",
    "\n",
    "try:\n",
    "    start_date_formatted = normalize_widget_date(raw_start_date)\n",
    "    end_date_formatted = normalize_widget_date(raw_end_date)\n",
    "    \n",
    "    print(f\"\\nPeríodo de Extração (API BCB):\")\n",
    "    print(f\"  Data de Início: {start_date_formatted}\")\n",
    "    print(f\"  Data Final:    {end_date_formatted}\")\n",
    "    \n",
    "except ValueError as e:\n",
    "    raise ValueError(f\"Falha na conversão de data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f578fc32-1705-4031-bf06-a5efc9d5b9fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Objetivo:**  \n",
    "Conectar-se à API oficial do Banco Central do Brasil (PTAX) para obter as cotações de compra do dólar dentro do intervalo de datas previamente configurado.\n",
    "\n",
    "**Descrição detalhada:**\n",
    "- **`ENDPOINT`** → constrói dinamicamente a URL da requisição, inserindo as datas de início e fim formatadas conforme o padrão exigido pela API (`MM-DD-YYYY`).  \n",
    "- **Requisição HTTP** → utiliza `requests.get()` com tempo limite de 30 segundos para garantir resposta dentro de um intervalo seguro.  \n",
    "  Caso ocorra erro de conexão, `raise_for_status()` força a interrupção e o tratamento da exceção.  \n",
    "- **`payload`** → armazena o conteúdo JSON retornado pela API. Em seguida, o código extrai os registros em `payload[\"value\"]`.  \n",
    "- **`df_pd`** → converte o conteúdo obtido em um `DataFrame` do pandas.  \n",
    "- Se **não houver dados**, um `Spark DataFrame` vazio é criado com as colunas `dataHoraCotacao` e `cotacaoCompra`.  \n",
    "- Se **houver dados**, a coluna `cotacaoCompra` é renomeada, e adiciona-se a coluna `ingestion_timestamp` para registrar o momento da ingestão.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb21f6e3-33cb-4dc7-9be1-9d9463a6509f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ENDPOINT = (\n",
    "    \"https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata/\"\n",
    "    f\"CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)?\"\n",
    "    f\"@dataInicial='{start_date_formatted}'&@dataFinalCotacao='{end_date_formatted}'\"\n",
    "    f\"&$select=dataHoraCotacao,cotacaoCompra&$format=json\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    resp = requests.get(ENDPOINT, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "except requests.exceptions.RequestException as e:\n",
    "    raise RuntimeError(f\"Falha ao acessar a API do BCB: {e}\")\n",
    "\n",
    "payload = resp.json()\n",
    "records = payload.get(\"value\", [])\n",
    "\n",
    "if not records:\n",
    "    print(\"API retornou 0 registros para o período. Nenhuma linha será inserida.\")\n",
    "\n",
    "df_pd = pd.DataFrame(records)\n",
    "\n",
    "if df_pd.shape[0] == 0:\n",
    "    spark_df = spark.createDataFrame([], schema='dataHoraCotacao:string, cotacaoCompra:string')\n",
    "    print(\"Nenhum dado novo para carregar.\")\n",
    "else:\n",
    "    if \"cotacaoCompra\" in df_pd.columns:\n",
    "        df_pd = df_pd.rename(columns={\"cotacaoCompra\": \"purchase_rate\"})\n",
    "    \n",
    "    if \"dataHoraCotacao\" not in df_pd.columns:\n",
    "         df_pd[\"dataHoraCotacao\"] = None \n",
    "\n",
    "    spark_df = spark.createDataFrame(df_pd)\n",
    "    spark_df = spark_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    \n",
    "    print(f\"Dados extraídos. Total de linhas prontas para carga: {spark_df.count()}\")\n",
    "    spark_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50a51446-c0d4-4cf0-a0d4-912d8085583e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Objetivo:**  \n",
    "Armazenar os dados extraídos da API do Banco Central na tabela `dm_cotacao_dolar` da camada Bronze, utilizando o formato Delta para garantir rastreabilidade e versionamento dos dados.\n",
    "\n",
    "**Descrição detalhada:**\n",
    "- **Verificação inicial:** caso o `DataFrame` esteja vazio (`spark_df.count() == 0`), o processo é encerrado exibindo uma mensagem informativa de que não há novas cotações para carregar.  \n",
    "- **Gravação em Delta:** se houver dados, o `DataFrame` é salvo com:\n",
    "  - **`format(\"delta\")`** → define o formato Delta Lake, padrão para camadas do Lakehouse;  \n",
    "  - **`mode(\"overwrite\")`** → sobrescreve completamente os dados anteriores, mantendo o schema atualizado;  \n",
    "  - **`option(\"overwriteSchema\", \"true\")`** → assegura que alterações no schema sejam refletidas na tabela existente;  \n",
    "  - **`saveAsTable(table_full_name)`** → grava os dados como tabela gerenciada no catálogo `medalhao.bronze`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f0e7db-5d81-44cb-a2b5-7211e7d3f9a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if spark_df.count() == 0:\n",
    "    print(\"Processo finalizado: Nenhuma nova cotação para carregar na Bronze.\")\n",
    "else:\n",
    "    spark_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(table_full_name)\n",
    "\n",
    "    print(f\"Dados da cotação salvos em {table_full_name} (novas linhas: {spark_df.count()})\")\n",
    "\n",
    "    spark.table(table_full_name).orderBy(col(\"dataHoraCotacao\").desc()).limit(10).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8001608875070154,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_land_to_bronze",
   "widgets": {
    "bcb_data_fim": {
     "currentValue": "",
     "nuid": "7c26d922-f8e2-434a-a5b1-e734f715d2da",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "bcb_data_fim",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "bcb_data_fim",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "bcb_data_inicio": {
     "currentValue": "",
     "nuid": "da3047a9-1e50-4f2e-aa5c-042e7587d06d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "bcb_data_inicio",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "bcb_data_inicio",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}